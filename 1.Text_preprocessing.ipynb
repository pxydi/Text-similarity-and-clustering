{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text is highly unstructured and needs to be prepared into a form that can be processed by machine learning algorithms. There are several different [approaches](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) for extracting features from text and we will explore a few of them in the next notebook. However, before extracting features from text, we need to \"preprocess\" it, i.e., \"clean\" and \"standardize\" it. This is because raw text can be \"messy\", especially when coming from social media platforms! We need to keep as many \"informative\" words as possible while discarding the \"uninformative\" ones. Removing unnecessary terms, i.e., the \"noise\", will improve our models' performance.\n",
    "\n",
    "I will use the [Sentiment140](http://help.sentiment140.com/for-students/) public twitter corpus. This dataset contains ~500 tweets, labeled as positive, negative, or neutral. The dataset is available in the *data* folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os, re, random, string\n",
    "from collections import defaultdict\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Load helper functions\n",
    "import tools\n",
    "\n",
    "DATA_PATH = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size:  (498, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load corpus (twitter dataset)\n",
    "\n",
    "path = os.path.join(DATA_PATH,'sentiment_140.csv')\n",
    "df   = pd.read_csv(path, header=None)\n",
    "\n",
    "# Rename columns\n",
    "df.columns = ['label','tweet']\n",
    "\n",
    "# Re-order columns\n",
    "df = df[['tweet','label']].copy()\n",
    "\n",
    "# Drop duplicates\n",
    "df = df.drop_duplicates('tweet')\n",
    "\n",
    "# Remove empty rows\n",
    "df = df.dropna()\n",
    "\n",
    "# Rename labels\n",
    "label_dict = {0:'neg', 2:'neutral', 4:'pos'}\n",
    "df['label'] = df['label'].replace(label_dict)\n",
    "\n",
    "# Reset index\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "print('Data size: ',df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>confirmed: it's Time Warner's fault, not Facebook's, that fb is taking about 3 minutes to load. so tempted to switch to verizon =/</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>Heading to San Francisco</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>@defsounds WTF is the point of deleting tweets if they can still be found in summize and searches? Twitter, please fix that. Thanks and bye</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>RT @jessverr I love the nerdy Stanford human biology videos - makes me miss school. http://bit.ly/13t7NR</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>SHOUT OUTS TO ALL EAST PALO ALTO FOR BEING IN THE BUILDIN KARIZMAKAZE 50CAL GTA! ALSO THANKS TO PROFITS OF DOOM UNIVERSAL HEMPZ CRACKA......</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                            tweet  \\\n",
       "397            confirmed: it's Time Warner's fault, not Facebook's, that fb is taking about 3 minutes to load. so tempted to switch to verizon =/   \n",
       "284                                                                                                                      Heading to San Francisco   \n",
       "234   @defsounds WTF is the point of deleting tweets if they can still be found in summize and searches? Twitter, please fix that. Thanks and bye   \n",
       "38                                       RT @jessverr I love the nerdy Stanford human biology videos - makes me miss school. http://bit.ly/13t7NR   \n",
       "80   SHOUT OUTS TO ALL EAST PALO ALTO FOR BEING IN THE BUILDIN KARIZMAKAZE 50CAL GTA! ALSO THANKS TO PROFITS OF DOOM UNIVERSAL HEMPZ CRACKA......   \n",
       "\n",
       "       label  \n",
       "397      neg  \n",
       "284  neutral  \n",
       "234      neg  \n",
       "38       pos  \n",
       "80       pos  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show a few samples\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different ways for text preprocessing, depending on the application and the text's language. Below are a few issues that I would like to address in this particular dataset.\n",
    "\n",
    "* remove URLs (e.g., http://bit.ly/19epAH, www.tinyurl.com/m595fk)\n",
    "* remove RT (stands for retweet)\n",
    "* remove Twitter usernames (e.g., @BlondeBroad)\n",
    "* remove hashtags (e.g. #Adidas -> Adidas)\n",
    "* remove punctuation. However, a few groupings, such as `:-), <3, : d`, etc., express emotion, so, depending on the task, we may want to keep them in the text.\n",
    "* remove numbers (e.g. 2020, 2, 15, ...)\n",
    "* perform case conversion (e.g. Good -> good, ...)\n",
    "* remove stopwords\n",
    "* remove non-ASCII characters\n",
    "* standardize the number of repeated characters, e.g. (\"I loooooooovvvvvveee\" -> \"I loovvee\")\n",
    "* expand contractions (e.g. \"don't\" -> \"do not\", \"won't\" -> \"will not\", ...)\n",
    "* apply stemming\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization is the process of splitting text into **words**.\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\\text{I came to Bern by train.} → \\text{[I, came, to, Bern, by, train.]}$$\n",
    "  \n",
    "  \n",
    "**The *TweetTokenizer***\n",
    "\n",
    "The *TweetTokenizer* is a nice tool from the NLTK library, specially designed for tokenizing tweets. Apart from spliting text into words, it offers a few additional key options:\n",
    "- reduces the number of repeated characters within a token e.g. \"everrrrr\" -> \"everrr\" (use: *reduce_len=True*)\n",
    "- removes Twitter usernames (use: *strip_handles=True*)\n",
    "- preserves punctuation and emoticons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Create an instance of the tokenizer\n",
    "tokenizer = TweetTokenizer(reduce_len=True, strip_handles=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <-- \n",
      "\n",
      "['This', 'is', 'a', 'coool', '#dummysmiley', ':', ':-)', ':-P', '<3', 'and', 'some', 'arrows', '<', '>', '->', '<--']\n"
     ]
    }
   ],
   "source": [
    "# Example using emoticons and punctuation\n",
    "\n",
    "sample_1 = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "\n",
    "print(sample_1,'\\n')\n",
    "print(tokenizer.tokenize(sample_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@remy: This is waaaaayyyy too much for you!!!!!! \n",
      "\n",
      "[':', 'This', 'is', 'waaayyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "# Example using strip_handles and reduce_len parameters\n",
    "\n",
    "sample_2 = '@remy: This is waaaaayyyy too much for you!!!!!!'\n",
    "\n",
    "print(sample_2,'\\n')\n",
    "print(tokenizer.tokenize(sample_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case folding\n",
    "\n",
    "We usually convert all documents to lowercase. This is because we want our models to count e.g. \"I\" together with \"i\", \"The\" together with \"the\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet: \t\t\tI don't like this movie!\n",
      "\n",
      "Convert to lowercase: \t\t\ti don't like this movie!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_3 = \"I don't like this movie!\"\n",
    "\n",
    "print('Original tweet: \\t\\t\\t{}\\n'.format(sample_3))\n",
    "\n",
    "# Case folding\n",
    "print('Convert to lowercase: \\t\\t\\t{}\\n'.format(sample_3.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Stopwords are words that are essential for a sentence to make sense, such as: \"I\", \"the\", \"and\", etc. The issue with stopwords is that they are: *very frequent* and *uninformative*. For most NLP applications, it is a good idea to remove them from text. \n",
    "\n",
    "Most NLP libraries provide pre-compiled lists of stopwords for several languages. In this notebook, we will use the list provided by the [NLTK library](https://www.nltk.org/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/xydi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load english stopwords from nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords          \n",
    "stopwords_english = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179 stopwords in NLTK's list.\n",
      "\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Print stopwords\n",
    "\n",
    "print('{} stopwords in NLTK\\'s list.\\n'.format(len(stopwords_english)))\n",
    "print(stopwords_english)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the NLTK stopword list includes negation words such as:\n",
    "\n",
    "- no, nor, not\n",
    "- don't, didn't, wouldn't\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if 'not' is in stopwords_english\n",
    "\n",
    "'not' in stopwords_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if \"don't\" is in stopwords_english\n",
    "\n",
    "\"don't\" in stopwords_english"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may be an important point to keep in mind, depending on the application. For example, if we use the NLTK stopword list \"out-of-the-box\", then a sentence like:\n",
    "\n",
    "$$\\text{\"I don't like this movie\"}$$\n",
    "\n",
    "will become:\n",
    "$$\\text{\"like movie\"}$$ \n",
    "\n",
    "We see that the processed sentence conveys the exact opposite sentiment from the original one! \n",
    "\n",
    "I will \"customize\" the NLTK stopword list to ensure that I don't remove negation words from tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove negation words from nltk's stopword list\n",
    "\n",
    "not_stopwords = {'no', 'nor', 'not'} \n",
    "custom_stopwords = set([word for word in stopwords_english if word not in not_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'these', 'doesn', 'ourselves', 'some', \"hasn't\", 'isn', 'any', 'further', 'few', \"it's\", 'there', 'll', 'shouldn', 'before', 'should', 'you', 'now', 'hadn', 'won', 's', \"aren't\", 'too', 'through', 'about', 'such', 'for', 'than', 've', 'the', 'its', 'after', 'o', 'will', 'only', 'me', 'being', 'on', 'd', 'him', 'didn', 'who', 'this', 'y', 'himself', 'from', \"don't\", 'they', 'their', 're', 'are', 'them', 'have', \"mightn't\", \"won't\", 'here', 'myself', 'in', 'until', 'hasn', 'haven', 'up', \"you'd\", 'were', \"you've\", 'off', 'been', 'over', 'wouldn', 'which', 'weren', 'yours', 'couldn', 'aren', 'to', 'i', 'she', 'did', 'into', \"haven't\", 'once', \"she's\", 'of', 'we', 'whom', 'between', 'ain', \"hadn't\", 'above', 'own', 'same', 'has', 'don', 'so', 'then', 'yourselves', 'or', 'with', 'where', 't', 'themselves', \"couldn't\", 'very', 'was', 'had', 'by', 'both', 'during', 'if', 'more', 'am', 'doing', 'other', 'but', 'can', 'herself', 'out', 'how', 'it', 'under', 'mustn', 'he', 'ma', 'my', 'and', \"shouldn't\", 'why', 'that', \"didn't\", 'do', \"wasn't\", 'what', \"you're\", 'her', 'because', 'most', 'mightn', 'shan', 'down', 'theirs', 'yourself', 'ours', \"that'll\", 'when', 'again', 'does', 'is', \"needn't\", \"weren't\", 'all', 'hers', \"mustn't\", 'his', 'against', 'below', 'm', 'be', 'each', 'wasn', \"you'll\", 'your', 'a', \"shan't\", 'while', 'at', \"should've\", \"isn't\", \"wouldn't\", 'itself', 'our', 'those', 'just', \"doesn't\", 'an', 'having', 'as', 'needn'}\n"
     ]
    }
   ],
   "source": [
    "print(custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if 'not' is in custom_stopwords\n",
    "\n",
    "'no' in custom_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, customizing the NLTK stopword lists will not be enough for preserving negation words in the text. We should also expand contractions: e.g. \"don't\" -> \"do not\". We can do this with the library \"contractions\".\n",
    "\n",
    "$$\\text{\"I don't like this movie\"}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet: \t\t\tI don't like this movie!\n",
      "\n",
      "Convert to lowercase: \t\t\ti don't like this movie!\n",
      "\n",
      "Expand contractions: \t\t\ti do not like this movie!\n",
      "\n",
      "Tokenize: \t\t\t\t['i', 'do', 'not', 'like', 'this', 'movie!']\n",
      "\n",
      "Remove stopwords: \t\t\t['not', 'like', 'movie!']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "\n",
    "sample_3 = \"I don't like this movie!\"\n",
    "print('Original tweet: \\t\\t\\t{}\\n'.format(sample_3))\n",
    "\n",
    "# Case folding\n",
    "print('Convert to lowercase: \\t\\t\\t{}\\n'.format(sample_3.lower()))\n",
    "\n",
    "# Expand contractions\n",
    "sample_3_expanded = contractions.fix(sample_3.lower())\n",
    "print('Expand contractions: \\t\\t\\t{}\\n'.format(sample_3_expanded))\n",
    "\n",
    "# Tokenize tweet\n",
    "print('Tokenize: \\t\\t\\t\\t{}\\n'.format(sample_3_expanded.split()))\n",
    "\n",
    "# Remove stopwords\n",
    "print('Remove stopwords: \\t\\t\\t{}\\n'.format([w for w in sample_3_expanded.split() if w not in custom_stopwords]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made sure that the \"important\" words for guessing the sentiment of this tweet (i.e. \"not\" and \"like\") were preserved!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Another part of text normalization is stemming, in which we mainly strip suffixes from the end of the word. The Porter stemmer is a widely used stemming tool for the English language. Stemming helps to connect words and reduce the size of the vocabulary (i.e., the number of unique words in a corpus). However, it can produce non-words, i.e., words that you won't find in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stemmer\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'retriev'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem of \"retrieve\"\n",
    "\n",
    "stemmer.stem('retrieve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'retriev'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem of \"retrieval\"\n",
    "\n",
    "stemmer.stem('retrieval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'retriev'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem of \"retrieved\"\n",
    "\n",
    "stemmer.stem('retrieved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The process_tweet function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's bring everything together and create the `process_tweet` funchion which takes a tweet as an argument and  preprocesses it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create process_tweet function\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    \n",
    "    '''\n",
    "    Preprocess raw samples of tweets.\n",
    "    \n",
    "    INPUT: \n",
    "    - tweet: raw text (string)\n",
    "    \n",
    "    OUTPUT:\n",
    "    - processed_tweet: processed tweet (string)\n",
    "    '''\n",
    "    \n",
    "    # Remove RT\n",
    "    clean_tweet = re.sub(r'RT','',tweet)\n",
    "\n",
    "    # Remove URL\n",
    "    clean_tweet = re.sub(r'https?:\\/\\/[^\\s]+','',clean_tweet)\n",
    "\n",
    "    # Remove hash #\n",
    "    clean_tweet = re.sub(r'#','',clean_tweet)\n",
    "\n",
    "    # Remove numbers\n",
    "    clean_tweet = re.sub(r'\\d+','',clean_tweet)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    clean_tweet = clean_tweet.lower()\n",
    "    \n",
    "    # Remove punctuation repetions (that are not removed by TweetTokenizer)\n",
    "    clean_tweet = re.sub(r'([._:]){2,}','',clean_tweet)\n",
    "    \n",
    "    # Remove non-ascii chars\n",
    "    clean_tweet = ''.join([c for c in str(clean_tweet) if ord(c) < 128])\n",
    "\n",
    "    # Expand contractions\n",
    "    clean_tweet = contractions.fix(clean_tweet)\n",
    "    \n",
    "    # Tokenize tweet\n",
    "    tokens = tokenizer.tokenize(clean_tweet)\n",
    "\n",
    "    # Remove punctuation (except emoticons), stopwords, single-char words and apply stemming\n",
    "    clean_tokens = [stemmer.stem(w) for w in tokens if (w not in string.punctuation) and\n",
    "                       (w not in custom_stopwords) and (len(w)>1)]\n",
    "    \n",
    "    # The stemmer strips the final 's but leaves the apostroph: warner's -> warner'\n",
    "    # Here, I'm removing the apostroph from the end of words\n",
    "    clean_tokens = [tok if tok[-1] != \"'\" else tok[:-1] for tok in clean_tokens]\n",
    "\n",
    "    # Join tokens in a single string to recreate the tweet\n",
    "    processed_tweet = ' '.join([tok for tok in clean_tokens])\n",
    "    processed_tweet = processed_tweet.strip()\n",
    "       \n",
    "    return processed_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this function with a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning: \tluke and i got stopped walking out of safeway and asked to empty our pockets and lift our shirts. how jacked up is that?\n",
      "\n",
      "After cleaning: \tluke got stop walk safeway ask empti pocket lift shirt jack\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample a tweet randomly from the corpus\n",
    "\n",
    "tweet =  df.iloc[random.randint(0,len(df)-1),0]\n",
    "\n",
    "print('Before cleaning: \\t{}\\n'.format(tweet))\n",
    "print('After cleaning: \\t{}\\n'.format(process_tweet(tweet)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now go ahead and process all tweets in the corpus using the `process_tweet` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all tweets (and add to list)\n",
    "\n",
    "processed_tweets = [process_tweet(tweet).split() for tweet in df['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['looovvvee', 'kindl', 'not', 'dx', 'cool', 'fantast', 'right'],\n",
       " ['read', 'kindl', 'love', 'lee', 'child', 'good', 'read'],\n",
       " ['ok', 'first', 'asses', 'kindl', 'fuck', 'rock']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show a few samples of processed tweets\n",
    "\n",
    "processed_tweets[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations\n",
    "\n",
    "There is one last thing we can do before moving on to feature extraction.\n",
    "\n",
    "We can look for combinations of words that frequently appear together, such as: \"Night at the museum\", \"Star Trek\", \"San Francisco\", \"North Korea\", etc, and replace them by a unique token. \n",
    "\n",
    "Example: \n",
    "\n",
    "$$\\text{\"Star Trek\" → \"Star_Trek\"}$$\n",
    "$$\\text{\"San Francisco\" → \"San_Francisco\"}$$\n",
    "$$\\text{\"North Korea\" → \"North_Korea\"}$$\n",
    "\n",
    "\n",
    "We often call these *phrases* or *collocations*; they are word combinations that are more common in the corpus than the individual words themselves. (*Note: \"that is\" is not considered a collocation*).\n",
    "\n",
    "I will use Gensim's `models.phrases` to detect phrases (collocations) in our corpus. \"Phrases\" will identify the most common collocations and join the constituent tokens into a single token, using the \"_\" glue character. \n",
    "\n",
    "*Documentation*\n",
    "* Gensim's website: https://radimrehurek.com/gensim/models/phrases.html\n",
    "* Mikolov, *et. al*: [\"Distributed Representations of Words and Phrases and their Compositionality\"](https://arxiv.org/pdf/1310.4546.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing collocations (bigrams)...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Detect collocations in corpus\n",
    "\n",
    "for i in ['bigrams']:\n",
    "    print('Computing collocations ({})...'.format(i))\n",
    "    \n",
    "    bigram = gensim.models.Phrases(processed_tweets,   # Expected format: list of tokenized documents\n",
    "                                   min_count=3,        # Ignore all words and bigrams with total collected count lower than this value.\n",
    "                                   delimiter=b'_')     # Glue character used to join collocation tokens\n",
    "\n",
    "    bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "    # Add detected collocations to corpus\n",
    "    processed_tweets = [' '.join(bigram_model[processed_tweet]) for processed_tweet in processed_tweets]\n",
    "    print('Done!')\n",
    "    \n",
    "# Add processed tweets to dataframe\n",
    "df['processed_tweet'] = processed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size:  (498, 3)\n"
     ]
    }
   ],
   "source": [
    "# Remove empty tweets\n",
    "df = df[df['processed_tweet'].apply(len) != 0].copy()\n",
    "\n",
    "# Reindex dataframe\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# Re-order columns\n",
    "df = df[['tweet','processed_tweet','label']]\n",
    "\n",
    "print('Data size: ',df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the processed tweets look like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>processed_tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>still hungry after eating....</td>\n",
       "      <td>still hungri eat</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>It's a bank holiday, yet I'm only out of work now. Exam season sucks:(</td>\n",
       "      <td>bank holiday yet work exam season suck :(</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>Portland city politics may undo baseball park http://tinyurl.com/lpjquj</td>\n",
       "      <td>portland citi polit may undo basebal park</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>I seriously underestimated Malcolm Gladwell.  I want to meet this dude.</td>\n",
       "      <td>serious underestim malcolm_gladwel want meet dude</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Going to the dentist later.:|</td>\n",
       "      <td>go dentist later</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       tweet  \\\n",
       "429                                            still hungry after eating....   \n",
       "99    It's a bank holiday, yet I'm only out of work now. Exam season sucks:(   \n",
       "415  Portland city politics may undo baseball park http://tinyurl.com/lpjquj   \n",
       "295  I seriously underestimated Malcolm Gladwell.  I want to meet this dude.   \n",
       "169                                            Going to the dentist later.:|   \n",
       "\n",
       "                                       processed_tweet    label  \n",
       "429                                   still hungri eat  neutral  \n",
       "99           bank holiday yet work exam season suck :(      neg  \n",
       "415          portland citi polit may undo basebal park  neutral  \n",
       "295  serious underestim malcolm_gladwel want meet dude      pos  \n",
       "169                                   go dentist later      neg  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually label tweets into topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later analysis, I will now label tweets manually based on selected keywords.\n",
    "\n",
    "For example, we will add any tweet mentioning movie-related terms (e.g. \"Night at the museum\", \"Star Trek\", \"movie\", etc.) to the category: \"movies\". Tweets mentioning politics-related terms (e.g. \"north_korea\", \"obama\", \"pelosi\", \"bush\", \"china\", \"india\", \"iran\", etc.) will be added to \"politics\". Tweets mentionning sports-related terms (e.g. \"lebron\", \"laker\", \"basebal\", \"basketbal\", \"fifa\", \"ncaa\", \"roger\", \"feder\", etc.) will be added to \"sports\". And so on.\n",
    "\n",
    "We will use the function `add_to_topic` to label tweets manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function add_to_topic() \n",
    "\n",
    "df = tools.add_to_topic(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>processed_tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>#at&amp;amp;t is complete fail.</td>\n",
       "      <td>complet fail</td>\n",
       "      <td>neg</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Night at the Museum, Wolverine and junk food - perfect monday!</td>\n",
       "      <td>night_museum wolverin junk food perfect monday</td>\n",
       "      <td>pos</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>DevSnippets : jQuery Tools - Javascript UI Components for the Web... http://inblogs.org/go/hfuqt</td>\n",
       "      <td>devsnippet jqueri tool javascript ui compon web</td>\n",
       "      <td>neutral</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>I hate revision, it's so boring! I am totally unprepared for my exam tomorrow :( Things are not looking good...</td>\n",
       "      <td>hate revis bore total unprepar exam tomorrow :( thing not look good</td>\n",
       "      <td>neg</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>I'm really loving the new search site Wolfram/Alpha. Makes Google seem so ... quaint. http://www72.wolframalpha.com/</td>\n",
       "      <td>realli love new search site wolfram alpha make googl seem quaint</td>\n",
       "      <td>pos</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                    tweet  \\\n",
       "223                                                                                           #at&amp;t is complete fail.   \n",
       "132                                                        Night at the Museum, Wolverine and junk food - perfect monday!   \n",
       "441                      DevSnippets : jQuery Tools - Javascript UI Components for the Web... http://inblogs.org/go/hfuqt   \n",
       "97        I hate revision, it's so boring! I am totally unprepared for my exam tomorrow :( Things are not looking good...   \n",
       "306  I'm really loving the new search site Wolfram/Alpha. Makes Google seem so ... quaint. http://www72.wolframalpha.com/   \n",
       "\n",
       "                                                         processed_tweet  \\\n",
       "223                                                         complet fail   \n",
       "132                       night_museum wolverin junk food perfect monday   \n",
       "441                      devsnippet jqueri tool javascript ui compon web   \n",
       "97   hate revis bore total unprepar exam tomorrow :( thing not look good   \n",
       "306     realli love new search site wolfram alpha make googl seem quaint   \n",
       "\n",
       "       label      topic  \n",
       "223      neg  unlabeled  \n",
       "132      pos       food  \n",
       "441  neutral         IT  \n",
       "97       neg  unlabeled  \n",
       "306      pos         IT  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show a few samples\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "\n",
    "#df.to_csv('clean_sentiment_140.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
